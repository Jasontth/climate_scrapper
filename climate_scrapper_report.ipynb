{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_adb_reports():\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.max_redirects = 20\n",
    "\n",
    "    df = pd.DataFrame(columns=['Title', 'Source', 'Date', 'Research Area', 'Keywords', 'Authors', 'Abstract', 'External Link', 'PDF Link'])\n",
    "\n",
    "    for i in range(54):\n",
    "        try:\n",
    "            url = \"https://www.adb.org/search0/language/en/subject/climate-change/type/institutional_document/type/publication?page=\" + str(i)\n",
    "            response = session.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            pdfs_url = []\n",
    "            for link in soup.find_all('a'):\n",
    "                if link.get('href').startswith(\"/publications/\"):\n",
    "                    extend_url = \"https://www.adb.org\" + link.get('href')\n",
    "                    pdfs_url.append(extend_url)\n",
    "            \n",
    "            for url in pdfs_url:\n",
    "                response = session.get(url, headers=headers)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                for link in soup.find_all('a'):\n",
    "                    if link.get('href').endswith(\".pdf\"):\n",
    "                        pdf_url = \"https://www.adb.org\" + link.get('href')\n",
    "\n",
    "                print(url)\n",
    "                title_tag = soup.find(\"meta\",  {\"name\":\"citation_title\"})\n",
    "                title = title_tag['content'] if title_tag else 'N/A'\n",
    "                source = 'Asian Development Bank'\n",
    "                date_tag = soup.find(\"meta\",  {\"name\":\"citation_publication_date\"})\n",
    "                date = date_tag['content'] if date_tag else 'N/A'\n",
    "                research_area = 'Climate Change'\n",
    "                keywords_tag = soup.find(\"meta\",  {\"name\":\"keywords\"})\n",
    "                keywords = keywords_tag['content'] if keywords_tag else 'N/A'\n",
    "                authors_tag = soup.find(\"meta\",  {\"name\":\"  \"})\n",
    "                authors = authors_tag['content'] if authors_tag else 'N/A'\n",
    "                abstract_tag = soup.find(\"meta\",  {\"name\":\"description\"})\n",
    "                abstract = abstract_tag['content'] if abstract_tag else 'N/A'\n",
    "                external_link = url\n",
    "                pdf_link = pdf_url\n",
    "                df.loc[len(df)] = [title, source, date, research_area, keywords, authors, abstract, external_link, pdf_link]\n",
    "                time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    df = df.drop_duplicates(subset=['Title', 'Source', 'Date', 'Research Area', 'Keywords', 'Authors', 'Abstract', 'External Link', 'PDF Link'], keep='first')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_iea_reports():\n",
    "    # The list of links to the reports\n",
    "    links_list = []\n",
    "\n",
    "    # Get the data from https://www.iea.org/analysis?type=report from page 0 to 63\n",
    "    for page in range(0, 64):\n",
    "        try:\n",
    "            url = 'https://www.iea.org/analysis?type=report&page=' + str(page)\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            links = [link['href'] for link in links if link['href'].startswith('/reports/')]\n",
    "            links = list(set(links))\n",
    "            links_list.extend(links)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while fetching links from page {page}: {e}\")\n",
    "\n",
    "    # dataframe to store report data\n",
    "    df = pd.DataFrame(columns=['title', 'date', 'summary', 'external_link', 'pdf_link', 'report_type'])\n",
    "\n",
    "    # Get the data from each report\n",
    "    for report_url in links_list:\n",
    "        try:\n",
    "            url = 'https://www.iea.org' + report_url\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # title\n",
    "            title = soup.find('title').text\n",
    "\n",
    "            # date\n",
    "            published_label = soup.find('span', class_='m-meta-infos__item-label', text='Published')\n",
    "            date = published_label.find_next_sibling('span', class_='m-meta-infos__item-value').text\n",
    "\n",
    "            # summary\n",
    "            div = soup.find('div', class_='m-report-abstract__desc f-rte')\n",
    "            paragraphs = div.find_all('p')\n",
    "            summary = ' '.join(paragraph.text for paragraph in paragraphs)\n",
    "            \n",
    "            # external_link\n",
    "            external_link = url\n",
    "\n",
    "            # pdf_link\n",
    "            pdf_link_tag = soup.find('a', href=lambda x: x and x.endswith('.pdf'))\n",
    "            pdf_link = pdf_link_tag['href'] if pdf_link_tag else None\n",
    "\n",
    "            # return a row\n",
    "            row = pd.DataFrame({'title': [title], 'date': [date], 'summary': [summary], 'external_link': [external_link], 'pdf_link': [pdf_link]})\n",
    "            print(row)\n",
    "\n",
    "            # append the row to the dataframe\n",
    "            df = pd.concat([df, row], ignore_index=True)\n",
    "            \n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while fetching data from report {report_url}: {e}\")\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
